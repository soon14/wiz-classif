
文本相似度的一种计算方法

李萌
自然语言处理&机器学习爱好者

本文是作者结合资料对论文From Word Embeddings To Document Distances的理解，其中有些地方理解不到位，还希望读者能批评指正。

本文首先会介绍BOW和TF-IDF，再介绍Word2Vec的词嵌套模型，然后介绍本论文的核心内容WMD，并且介绍本文提出的两种优化速度的方法，最后提出一些自己的想法并做一总结。

一 BOW和TF-IDF
在自然语言处理中，文本相似度是一个很重要的领域，并且其商业应用也很广阔，包括文本检索、新闻分类与聚类 、歌曲识别、多语言文档匹配等，都可以用文本相似度来评判。现在比较常见的文本间距离度量为BOW和TF-IDF，我们简单介绍下BOW和TF-IDF。

1. BOW
BOW全名叫Bag of words，也称为词袋模型，在信息检索中，BOW假定对于一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词 是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。

那么在文本相似度中，词袋模型是将两篇文本通过词袋模型变为向量模型，通过计算向量的余弦距离来计算两个文本间的相似度。但这有个明显的缺点，就是严重缺乏相似词之间的表达。比如“我喜欢足球”和“我不喜欢足球”这两个文本是严重不相似的，但词袋模型会判为高度相似。再比如“我喜欢足球”和“我热爱足球”的意思是非常的接近的，但词袋模型不能表示“喜欢”和“爱”之间严重的相似关系。

2. TF-IDF
TF-IDF的全称是term frequency–inverse document frequency，翻译为词频-逆向文件频率。TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

TF-IDF和BOW的缺点相同，无法用于描述词与词之间的关系，从原理上来看，这两种方法是专注与单个词的，而不是整个文本相似度的，所以用这两个方法做文本相似度的度量肯定效果不会太好。

二 Word2Vec
论文的作者就提出了一种新的方法，该方法也是计算两文本间距离矩阵，而不同的是这个新的方法是基于Word2Vec的词嵌套来计算的，而且该方法很好的解决了之前两种方法不足的地方。那究竟什么是Word2Vec呢？我们下面介绍：

Word2Vec是一种将文本中的词进行嵌入的方法，而所谓嵌入，就是将各个词使用一个定长的向量来表示。word2vec工具主要包含两个模型：连续词袋模型（continuous bag of words，简称CBOW）和跳字模型（skip-gram），word2vec词向量可以较好地表达不同词之间的相似和类比关系


左边是CBOW模型，它的目标是：给定一串文本，使用某个词的上下文来预测这个词。例如，对于句子“Old soldiers never die，they just fade away”，预测soldiers这个词时，可以使用never、die、fade、away这四个词，它们构成了soldiers的上下文。这样，从文本中就可以提取一系列的训练样本。得到上下文以后，将这些词的one-hot编码累加起来，输入神经网络，然后神经网络进行一些变换，其目标为得到fox这个词的one-hot编码。训练好之后的神经网络中，隐藏层的权重矩阵就是词嵌入后的向量了。

右边的skip-gram是类似的，它输入当前词soldiers，目标是变换为上下文的词汇，此处上下文词汇的处理与CBOW一样，也是将这些词的one-hot编码相加，作为训练使用的真值。

由上面的简单介绍可以了解到，将文本用word2vec模型进行训练，得出的词向量是具有词与词之间关系的，语义关系会保存在词向量的运算中。这是之前提到的两种模型不具备的。

三 WMD
论文作者提出的新方法，称为词移距离Word Mover's Distance (WMD)，就是利用了word2vec嵌入的这个属性。将文本文档表示为嵌入单词的加权点云。两个文本文档A和B之间的距离是文档A中的单词精确匹配文档B的点云所需要经过的最小累计距离。

WMD的模型基于EMD（Earth Mover Distance）模型。EMD和欧式距离一样，它们都是一种距离度量的定义、可以用来测量某两个分布之间的距离。

1. EMD

这里定义，货物从工厂Pi运送到仓库Qj的距离是dij，运送的货物质量为fij。这样运送一次的工作量为dij*fij，可见距离越远或者货物越重，工作量就越大。（运送货物可能有一对多，或者多对一的情况）。经过一些计算优化，这是得到工作量总和的最小值W。


距离dij是事先就存在的，可能是矩阵，所以运输量fij是式中唯一的变量，对fij做如下约束：

（1） 运输过程只能从工厂P到仓库Q，不能反向。


（2） 从工厂Pi运送出去的货物总和不能超过该工厂的所有货物的总质量WPi


（3） 仓库Qj接收的货物总质量不能超过其最大容量WQj


（4） 总运输量的上限是工厂中货物总重、仓库总容量中的最小值


解上面的式子就可以得到最优解fij。为了使EMD不会随着总运输量的变化而变化，每次的运输量还要除以总质量，以达到归一化的目的。


我们可以联想到将这种方法应用到文本相似度的识别中来，但是有个问题，我们怎么知道dij呢？这里我们就用到了上面介绍到的word2vec的词嵌套，它完美的符合完美的需求，因为本身具有语义关系，所有可以作为衡量距离。通过词嵌入（Word Embedding），我们可以得到词语的分布式低维实数向量表示，我们可以计算词语之间的距离，即我们可以得到dij，因此可以将EMD引入自然语言处理领域。

2. WMD
下面就将EMD用到文本相似度比较上。首先我们要用到nBOW(normalized bag-of-words)词袋表示，这个是归一化后的词袋，我用di表示权重，在这里我们去掉了停用词，只保留有价值的词，其中ci表示词语i在文档中中出现了ci次，分母为所有有用词的个数。


假设现在有两个文本，

文本一为 “Obama speaks to the media in Illinois”

文本二为 “The President greets the press in Chicago”

它们经过归一化处理后会是以下的样子:


接着通过欧几里得距离公式在word2vec中，计算两个单词i和j的距离，如下：


其中X输于word2vec的词嵌套矩阵，c(i,j)表示从单词i到单词j需要的距离。

转移量fij用矩阵T表示，生成的矩阵T如下：


前面已经提到了WMD就是EMD的变种，所以最终要求的最小化公式为：


其中di表示文本一，dj表示文本二，T>0表示只能从文本一到文本二，不能反向。

通过这个方法我们是要计算出T这个矩阵的，然后把每个维度的最大值累加，就可以表示文本一到文本二的相似度了，再进行比较。


由上图，比较D1、D2和D3对D0的相似度，我们先对D1和D2做WMD的一系列处理，如果我们要比较D1与D0的相似度，首先对D1和D0中去掉停止词，然后归一化(nBOW),得到在D0中 President, greets, press, Chicago的di都为0.25，图中从单词i在D1和D2到单词j在D0的箭头标记了他们在语义上的相似程度，即距离(距离越近相似性越高)，可以看到media比concert的意思更接近press，其他词以此类推，由此我们可以认为D1的意思更接近D0。在看D3，由于D3的单词c数和D0不匹配，导致D3的dj为0.33，这会使得在计算中增加距离，虽然文本较短，但是对应文本中可能存在多个相似的单词，导致最后的效果不佳。

3. 两种优化算法
这种方法能很好的比较两文本的相似度，但是时间复杂度很大，为O(p 3 logp)，因此作者提出了两种快速计算距离的方法，WCD（Word centroid distance）和RWMD(Relaxed word moving distance)，这两种算法是通过降低精度来降低计算复杂度的

先介绍WCD:

我们可以将WMD做如下近似变化，得到WCD：


WCD就是最后一个得到后面的式子，WCD的复杂度为O(dp)，其中X是一个d*p的矩阵，d表示词向量的维度，p表示nBOW模型的长度。

然后介绍RWMD，RWMD是通过放松限制条件，得到WMD的下限.

当我们去掉条件2，保留条件1时，记为L1，去掉条件2，可以理解为去掉仓库容量的限制，我们可以将货物全部运到离其最近的仓库，而不需要考虑仓库的容量。我们在运某个货物时，往离该货物最近的仓库运送，即在转移词语i时，我们只向与词语i距离最近的词语j转移。


我们会得到与单词i词移最相似的j，以及得到整个文本的转移矩阵T:


当我们去掉条件1，保留条件2时，去掉条件1，其实是去掉了货物量的限制，我们可以将货物源源不断的运到仓库中，直到仓库满了为止。我们在为填满某个仓库选择运送的货物时，选择离该仓库距离最近的货物，即在词语j接收时，我们选择接收与词语j距离近词语i。

当我们去掉条件1，保留条件2时,记为L2，去掉条件1，其实是去掉了货物量的限制，我们可以将货物源源不断的运到仓库中，直到仓库满了为止。我们在为填满某个仓库选择运送的货物时，选择离该仓库距离最近的货物，即在词语j接收时，我们选择接收与词语j距离近词语i。


这种算法的时间复杂度为O(p^2)，而且比WCD更严谨

4. 实验效果
以下是在八个文本集中的测试情况：可以看到在测试中WMD是全面占优的。



四 总结
1. 自己的想法：
在本片讨论的论文中，用到的是word2vec中的skip-gram模型，能不能用CBOW模型或者干脆用GloVe进行词嵌套，然后按照后面的进行，是否可行，效果会如何？

2. 总结
这篇论文提出的WMD再实际问题中很有应用价值，并且提出了优化时间复杂度的方法，而且在测试中也全面占优，所以这种算法应用在文本相似度上是高效可靠的，也为我们提供了一种优化的方法，通过改变下限或者多个限制条件组合来优化算法复杂度



参考：
From Word Embeddings To Document Distances
​
proceedings.mlr.press

https://blog.csdn.net/cht5600/article/details/53405315
​
blog.csdn.net


编辑于 2019-07-29
自然语言处理
文本分析
文本分类